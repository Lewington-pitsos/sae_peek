{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden: 71\n",
      "Trump: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6052 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/us_2020_election_speeches.csv')\n",
    "biden = df[df['speaker'] == 'Joe Biden']\n",
    "trump = df[df['speaker'] == 'Donald Trump']\n",
    "\n",
    "print('Biden:', biden.shape[0])\n",
    "print('Trump:', trump.shape[0])\n",
    "\n",
    "biden_texts = biden['text'].tolist()\n",
    "trump_texts = trump['text'].tolist()\n",
    "\n",
    "# Initialize GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize_and_segment(texts, tokenizer, max_length=256, overlap=5):\n",
    "    segments = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)['input_ids']\n",
    "        for i in range(0, len(tokens), max_length - overlap):\n",
    "            segment = tokens[i:i + max_length]\n",
    "            attention_mask = [1] * len(segment)\n",
    "            if len(segment) < max_length:\n",
    "                padding = [0] * (max_length - len(segment))\n",
    "                segment += padding\n",
    "                attention_mask += padding\n",
    "            segments.append({\"input_ids\": segment, \"attention_mask\": attention_mask})\n",
    "    return segments\n",
    "\n",
    "# Tokenize and segment speeches\n",
    "biden_segments = tokenize_and_segment(biden_texts, tokenizer)\n",
    "trump_segments = tokenize_and_segment(trump_texts, tokenizer)\n",
    "\n",
    "biden_dataset = Dataset.from_list(biden_segments)\n",
    "trump_dataset = Dataset.from_list(trump_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Trump: (00:30)\n",
      "Thank you. What a nice group. Thank you very much. Beautiful, thank you.\n",
      "Crowd: (00:50)\n",
      "Four more years.\n",
      "President Trump: (00:52)\n",
      "Thank you very much, please. We are going to be talking to our great senior citizens, that’s what I’m here for today.\n",
      "Speaker 1: (00:55)\n",
      "[crosstalk 00:00:50].\n",
      "President Trump: (00:56)\n",
      "We love our senior citizens. And I’m honored to be here in Fort Myers to reaffirm my solemn pledge to America’s seniors, it’s so important to me, I happen to be a senior. I will protect you, I will defend you, and I will fight for you with every ounce of energy and conviction that I have. You devoted your life to this country and I am devoting my life to you. My administration is working every day to give our amazing senior citizens the care, support, and respect that you deserve, and you understand that, we’ve worked together for a long time. As president, I’m deeply aware that America\n",
      "\n",
      "m deeply aware that America’s 54 million seniors have borne the heaviest burden of the China virus. Many older Americans have endured months of isolation, missing weddings, births, graduations, church, and family reunions, you know that very well, you know it all too well. My heart breaks for every grieving family that has lost a precious loved one, I feel their anguish, and I mourn their loss, I feel their pain.\n",
      "President Trump: (02:09)\n",
      "I know that the terrible pain that they have gone through, and you lose someone, and there’s nothing to describe what you have to bear, there’s nothing to describe it. In times of challenge, we turn to our fellow Americans for a shoulder to lean on and we turn to God for healing and strength, and together we will overcome. My message to America’s seniors today is one of optimism, confidence, and hope. Your sacrifice has not been in vain. The light at the end of the tunnel is near. We are rounding the turn, I say that all the time, some of the media doesn’t like hearing it, but I say it all the time, we’re rounding that turn. Don’t listen\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 340, 5692]), tensor([447,  13]), tensor([ 247, 1406]), tensor([ 82, 674]), tensor([  257, 10452]), tensor([ 369, 7320]), tensor([46859, 32675]), tensor([  13, 7557]), tensor([1318, 6971]), tensor([447, 290]), tensor([ 247, 4896]), tensor([ 82, 287]), tensor([257, 257]), tensor([1256, 1402]), tensor([  356, 11554]), tensor([714, 286]), tensor([1561, 3124]), tensor([ 546, 6095]), tensor([287, 284]), tensor([2846, 5068]), tensor([ 286, 1096]), tensor([262, 257]), tensor([23514,   649]), tensor([ 326, 3037]), tensor([3284,  329]), tensor([ 318, 1672]), tensor([1804,   13]), tensor([ 290, 1320]), tensor([ 407, 5419]), tensor([1804,  262]), tensor([  340, 11554]), tensor([ 13, 651]), tensor([ 887, 2067]), tensor([618,  13]), tensor([ 345, 3244]), tensor([ 466, 2839]), tensor([  11, 7713]), tensor([477,  11]), tensor([340, 356]), tensor([857, 760]), tensor([318,  11]), tensor([16637,  4003]), tensor([644, 262]), tensor([ 661, 6991]), tensor([892, 286]), tensor([743, 326]), tensor([ 307, 1597]), tensor([9829,  290]), tensor([  13, 1325]), tensor([1867,  511]), tensor([ 314, 2839]), tensor([ 836, 5054]), tensor([447, 355]), tensor([247, 880]), tensor([83, 13]), tensor([ 765, 1320]), tensor([ 284, 5419]), tensor([  423, 11372]), tensor([1645, 5046]), tensor([ 11, 290]), tensor([ 314, 1663]), tensor([836,  13]), tensor([ 447, 1320]), tensor([247, 447]), tensor([ 83, 247]), tensor([765,  82]), tensor([661, 703]), tensor([8978,  356]), tensor([262, 447]), tensor([966, 247]), tensor([810, 297]), tensor([345, 787]), tensor([ 460, 1654]), tensor([1833,  326]), tensor([3360,  883]), tensor([ 13, 389]), tensor([632, 262]), tensor([ 447, 1266]), tensor([ 247, 4213]), tensor([ 82, 389]), tensor([407, 407]), tensor([2861, 6699]), tensor([6709,  262]), tensor([   13, 13189]), tensor([ 198, 3139]), tensor([19585,   393]), tensor([21010, 15435]), tensor([ 25, 484]), tensor([357, 761]), tensor([2780,  780]), tensor([ 25, 286]), tensor([1821, 3234]), tensor([  8, 393]), tensor([  198, 19974]), tensor([1026, 2438]), tensor([2331,   13]), tensor([284, 198]), tensor([  502, 19585]), tensor([  326, 21010]), tensor([530,  25]), tensor([286, 357]), tensor([ 262, 1828]), tensor([1243,   25]), tensor([ 326, 2624]), tensor([314,   8]), tensor([1394,  198]), tensor([2111, 1870]), tensor([284, 994]), tensor([910, 447]), tensor([284, 247]), tensor([661,  82]), tensor([ 318, 1521]), tensor([ 11, 340]), tensor([ 564, 6067]), tensor([250,  13]), tensor([5247, 6498]), tensor([284, 783]), tensor([ 11, 356]), tensor([314, 447]), tensor([10594,   247]), tensor([27257,   260]), tensor([ 13, 287]), tensor([785, 262]), tensor([   13, 15925]), tensor([447, 286]), tensor([251, 530]), tensor([314, 286]), tensor([10594,   262]), tensor([27257,  6000]), tensor([  13, 7432]), tensor([785, 284]), tensor([ 290, 1402]), tensor([3785, 5692]), tensor([503, 674]), tensor([ 810, 1499]), tensor([345, 468]), tensor([ 460, 1683]), tensor([3015, 1775]), tensor([11, 13]), tensor([ 703, 1867]), tensor([ 345, 3759]), tensor([ 460, 1301]), tensor([3015, 1804]), tensor([ 13, 546]), tensor([1867,  340]), tensor([447,  30]), tensor([ 247, 3894]), tensor([82, 11]), tensor([1695,  339]), tensor([284, 447]), tensor([345, 247]), tensor([11, 82]), tensor([ 810, 1813]), tensor([ 345, 1263]), tensor([ 714, 6341]), tensor([3015,  262]), tensor([1903, 4077]), tensor([  11, 1657]), tensor([810, 284]), tensor([ 345, 8063]), tensor([ 714, 5242]), tensor([3015,  286]), tensor([ 287, 5054]), tensor([1048,  326]), tensor([ 11, 484]), tensor([810, 447]), tensor([340, 247]), tensor([447, 260]), tensor([ 247, 5017]), tensor([ 82, 329]), tensor([1016,  416]), tensor([284, 262]), tensor([ 307, 2717]), tensor([3338, 1230]), tensor([329,  11]), tensor([345, 290]), tensor([ 11, 787]), tensor([2123, 5242]), tensor([269, 286]), tensor([2357, 5054]), tensor([ 64, 287]), tensor([  13, 6642]), tensor([5224,  416]), tensor([  783, 43567]), tensor([ 11, 511]), tensor([1410,  749]), tensor([783, 880]), tensor([13, 12]), tensor([ 887, 2364]), tensor([262, 290]), tensor([517, 880]), tensor([11918,    12]), tensor([  326, 15236]), tensor([ 318, 7534]), tensor([384, 981]), tensor([  675, 25136]), tensor([994, 262]), tensor([  11, 3420]), tensor([340, 319]), tensor([ 447, 4833]), tensor([247,  11]), tensor([  82, 2042]), tensor([1016,  290]), tensor([ 284, 7586]), tensor([ 595, 1597]), tensor([ 259, 1231]), tensor([565, 777]), tensor([ 500, 8787]), tensor([661,  13]), tensor([284, 198]), tensor([  905, 19585]), tensor([  510, 21010]), tensor([13, 25]), tensor([843, 357]), tensor([ 287, 1954]), tensor([257,  25]), tensor([2565, 2999]), tensor([11,  8]), tensor([790, 198]), tensor([ 640, 1639]), tensor([314, 477]), tensor([2740, 3505]), tensor([546,  11]), tensor([340, 617]), tensor([ 11, 286]), tensor([314, 345]), tensor([1254, 5017]), tensor([588, 618]), tensor([314, 314]), tensor([447, 717]), tensor([ 247, 8104]), tensor([ 76, 503]), tensor([2712,  644]), tensor([656, 314]), tensor([ 465, 1807]), tensor([ 983, 2622]), tensor([ 13, 284]), tensor([1892,  307]), tensor([ 466, 1760]), tensor([345, 287]), tensor([4236,  262]), tensor([351, 717]), tensor([ 502, 7628]), tensor([  11, 1410]), tensor([ 475, 1234]), tensor([ 857, 2651]), tensor([326, 416]), tensor([787, 262]), tensor([2565, 3162]), tensor([644,  11]), tensor([314, 314]), tensor([447, 531]), tensor([247,  11]), tensor([ 76, 356]), tensor([2282,  815]), tensor([ 30, 779])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/sae_peek/v/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3906\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3903\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/sae_peek/v/lib/python3.10/site-packages/transformers/tokenization_utils.py:1072\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1064\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1069\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1072\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1074\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1075\u001b[0m     }\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m~/code/sae_peek/v/lib/python3.10/site-packages/transformers/tokenization_utils.py:1047\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1045\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m-> 1047\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glance import Corpus\n",
    "\n",
    "biden_activations = Corpus('data/speeches-biden')\n",
    "trump_activations = Corpus('data/speeches-trump')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
